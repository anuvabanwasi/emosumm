{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Training and testing on original COVIDET dataset without including GPT-generated data"
      ],
      "metadata": {
        "id": "t92zwpTabSZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert-score"
      ],
      "metadata": {
        "id": "lPN2tGmCbWtv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a2b1c73-0ef7-40d7-dac1-d052332d64e6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bert-score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert-score) (2.1.0+cu121)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert-score) (1.5.3)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert-score) (4.35.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert-score) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert-score) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert-score) (4.66.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert-score) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert-score) (23.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2023.3.post1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (0.19.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (0.4.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (2023.11.17)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.0.1->bert-score) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->bert-score) (1.3.0)\n",
            "Installing collected packages: bert-score\n",
            "Successfully installed bert-score-0.3.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "hfswV1mzbWwU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8323b8f-2ee2-41b6-d435-f7330b37af76"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Collecting pyarrow-hotfix (from datasets)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: pyarrow-hotfix, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.15.0 dill-0.3.7 multiprocess-0.70.15 pyarrow-hotfix-0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "id": "Ik73Pox6bWyp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "818e3c24-0768-4f8e-edbe-acd4e7ed8d6c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.23.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=840307fc2d82a8b18734956f3ae80f902a186117680aa8c658ab691464ce1f2e\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install psaw"
      ],
      "metadata": {
        "id": "xIGs7rQfbW1A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7f59d5e-6bb3-43fa-c96a-a749c038f946"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting psaw\n",
            "  Downloading psaw-0.1.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from psaw) (2.31.0)\n",
            "Requirement already satisfied: Click in /usr/local/lib/python3.10/dist-packages (from psaw) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->psaw) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->psaw) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->psaw) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->psaw) (2023.11.17)\n",
            "Installing collected packages: psaw\n",
            "Successfully installed psaw-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bert_score import score as bert_scr\n",
        "import pandas as pd\n",
        "from absl import app\n",
        "from absl import flags\n",
        "\n",
        "from datasets import load_metric\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoModel, AutoTokenizer, DataCollatorForSeq2Seq, BartForConditionalGeneration\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "bgq1yJ4cbW3W"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To use the code, please first expand the Json files in the train_val_test directory by adding a Post key (along with the Reddit post text obtained using the PSAW wrapper) to each entry.\n",
        "\n",
        "# Directory path containing JSON files\n",
        "directory_path = \"./data/train_val_test_anonymized-WITH_POSTS\"\n",
        "\n",
        "# Iterate through each JSON file in the directory\n",
        "for filename in os.listdir(directory_path):\n",
        "    if filename.endswith(\".json\"):\n",
        "        file_path = os.path.join(directory_path, filename)\n",
        "\n",
        "        # Read the existing JSON data from the file\n",
        "        with open(file_path, \"r\") as file:\n",
        "            data = json.load(file)\n",
        "\n",
        "        # Iterate through each entry in the JSON data\n",
        "        for key, entry in data.items():\n",
        "            # Add the \"Post\" key with the Reddit post text\n",
        "            post_text = entry.get(\"Reddit Post\", \"\")\n",
        "            entry[\"Post\"] = post_text\n",
        "\n",
        "        # Write the modified JSON data back to the file\n",
        "        with open(file_path, \"w\") as file:\n",
        "            json.dump(data, file, indent=2)\n",
        "\n",
        "print(\"Post key added to each entry in JSON files.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7a_PfxlbeHr",
        "outputId": "ef890045-e37a-4b1b-c96b-a228c6333fc7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Post key added to each entry in JSON files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "3UrQiQeFf2Za"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!export 'PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512'"
      ],
      "metadata": {
        "id": "Qtdb8i8zn9Ms"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!TOKENIZERS_PARALLELISM=false python emotion_summarization.py --emotion \"disgust\" --training_path /content/data/train_val_test_anonymized-WITH_POSTS/train_anonymized-WITH_POSTS.json --validation_path /content/data/train_val_test_anonymized-WITH_POSTS/val_anonymized-WITH_POSTS.json --test_path /content/data/train_val_test_anonymized-WITH_POSTS/test_anonymized-WITH_POSTS.json --model facebook/bart-large-cnn --batch_size 1 --gradient_accumulation_steps 1 --results_summarization \"results\" --learning_rate 0.01"
      ],
      "metadata": {
        "id": "_T__96kKbePX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30795a56-4a36-4e4e-a709-b259715c33cf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-20 08:18:22.708822: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-20 08:18:22.708870: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-20 08:18:22.710215: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-20 08:18:23.929408: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/emotion_summarization.py:38: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  rouge_metric = load_metric(\"rouge\")\n",
            "config.json: 100% 1.58k/1.58k [00:00<00:00, 7.68MB/s]\n",
            "vocab.json: 100% 899k/899k [00:00<00:00, 2.70MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 1.85MB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 15.4MB/s]\n",
            "Map: 100% 227/227 [00:00<00:00, 951.74 examples/s]\n",
            "Map: 100% 192/192 [00:00<00:00, 1086.76 examples/s]\n",
            "Map: 100% 192/192 [00:00<00:00, 1097.85 examples/s]\n",
            "model.safetensors: 100% 1.63G/1.63G [00:04<00:00, 365MB/s]\n",
            "generation_config.json: 100% 363/363 [00:00<00:00, 2.28MB/s]\n",
            "100% 24/24 [01:37<00:00,  4.05s/it]\n",
            "tokenizer_config.json: 100% 52.0/52.0 [00:00<00:00, 277kB/s]\n",
            "config.json: 100% 792/792 [00:00<00:00, 2.91MB/s]\n",
            "vocab.json: 100% 899k/899k [00:00<00:00, 2.76MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 1.87MB/s]\n",
            "pytorch_model.bin: 100% 3.04G/3.04G [00:08<00:00, 343MB/s]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 7/7 [00:03<00:00,  2.01it/s]\n",
            "computing greedy matching.\n",
            "100% 4/4 [00:00<00:00, 22.52it/s]\n",
            "done in 3.67 seconds, 61.92 sentences/sec\n",
            "I1220 08:20:42.208054 135454596289152 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 24/24 [01:36<00:00,  4.03s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 7/7 [00:03<00:00,  2.19it/s]\n",
            "computing greedy matching.\n",
            "100% 4/4 [00:00<00:00, 51.02it/s]\n",
            "done in 3.28 seconds, 69.14 sentences/sec\n",
            "I1220 08:22:30.715696 135454596289152 rouge_scorer.py:83] Using default tokenizer.\n",
            "  0% 0/227 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 227/227 [00:30<00:00,  7.33it/s]\n",
            "100% 24/24 [00:53<00:00,  2.24s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 4/4 [00:01<00:00,  3.61it/s]\n",
            "computing greedy matching.\n",
            "100% 4/4 [00:00<00:00, 84.93it/s]\n",
            "done in 1.16 seconds, 195.63 sentences/sec\n",
            "I1220 08:24:05.703802 135454596289152 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 24/24 [00:54<00:00,  2.25s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 4/4 [00:01<00:00,  3.71it/s]\n",
            "computing greedy matching.\n",
            "100% 4/4 [00:00<00:00, 89.19it/s]\n",
            "done in 1.13 seconds, 201.22 sentences/sec\n",
            "I1220 08:25:07.988012 135454596289152 rouge_scorer.py:83] Using default tokenizer.\n",
            "  0% 0/227 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 227/227 [00:29<00:00,  7.62it/s]\n",
            "100% 24/24 [00:52<00:00,  2.18s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 6/6 [00:01<00:00,  3.50it/s]\n",
            "computing greedy matching.\n",
            "100% 4/4 [00:00<00:00, 72.75it/s]\n",
            "done in 1.77 seconds, 127.95 sentences/sec\n",
            "I1220 08:26:40.941871 135454596289152 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 24/24 [00:52<00:00,  2.18s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 6/6 [00:01<00:00,  3.52it/s]\n",
            "computing greedy matching.\n",
            "100% 4/4 [00:00<00:00, 73.32it/s]\n",
            "done in 1.76 seconds, 128.70 sentences/sec\n",
            "I1220 08:27:42.237897 135454596289152 rouge_scorer.py:83] Using default tokenizer.\n",
            "  0% 0/227 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 227/227 [00:29<00:00,  7.67it/s]\n",
            "100% 24/24 [00:51<00:00,  2.16s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 5/5 [00:01<00:00,  3.47it/s]\n",
            "computing greedy matching.\n",
            "100% 4/4 [00:00<00:00, 72.56it/s]\n",
            "done in 1.50 seconds, 151.32 sentences/sec\n",
            "I1220 08:29:13.427350 135454596289152 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 24/24 [00:51<00:00,  2.14s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 5/5 [00:01<00:00,  3.52it/s]\n",
            "computing greedy matching.\n",
            "100% 4/4 [00:00<00:00, 71.42it/s]\n",
            "done in 1.48 seconds, 153.38 sentences/sec\n",
            "I1220 08:30:13.498694 135454596289152 rouge_scorer.py:83] Using default tokenizer.\n",
            "  0% 0/227 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 227/227 [00:29<00:00,  7.67it/s]\n",
            "100% 24/24 [00:51<00:00,  2.15s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 5/5 [00:01<00:00,  3.10it/s]\n",
            "computing greedy matching.\n",
            "100% 4/4 [00:00<00:00, 74.73it/s]\n",
            "done in 1.67 seconds, 135.90 sentences/sec\n",
            "I1220 08:31:45.704169 135454596289152 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 24/24 [00:51<00:00,  2.14s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 5/5 [00:01<00:00,  3.13it/s]\n",
            "computing greedy matching.\n",
            "100% 4/4 [00:00<00:00, 72.60it/s]\n",
            "done in 1.66 seconds, 137.11 sentences/sec\n",
            "I1220 08:32:45.936105 135454596289152 rouge_scorer.py:83] Using default tokenizer.\n",
            "  0% 0/227 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 227/227 [00:29<00:00,  7.60it/s]\n",
            "100% 24/24 [00:51<00:00,  2.14s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 6/6 [00:01<00:00,  3.53it/s]\n",
            "computing greedy matching.\n",
            "100% 4/4 [00:00<00:00, 69.62it/s]\n",
            "done in 1.76 seconds, 128.84 sentences/sec\n",
            "I1220 08:34:17.429195 135454596289152 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 24/24 [00:51<00:00,  2.15s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 6/6 [00:01<00:00,  3.57it/s]\n",
            "computing greedy matching.\n",
            "100% 4/4 [00:00<00:00, 72.11it/s]\n",
            "done in 1.74 seconds, 130.37 sentences/sec\n",
            "I1220 08:35:17.971700 135454596289152 rouge_scorer.py:83] Using default tokenizer.\n",
            "  0% 0/227 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 227/227 [00:29<00:00,  7.70it/s]\n",
            "100% 24/24 [00:52<00:00,  2.19s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 4/4 [00:01<00:00,  3.66it/s]\n",
            "computing greedy matching.\n",
            "100% 4/4 [00:00<00:00, 90.99it/s]\n",
            "done in 1.14 seconds, 199.00 sentences/sec\n",
            "I1220 08:36:49.328724 135454596289152 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 24/24 [00:53<00:00,  2.21s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 4/4 [00:01<00:00,  3.66it/s]\n",
            "computing greedy matching.\n",
            "100% 4/4 [00:00<00:00, 70.95it/s]\n",
            "done in 1.15 seconds, 196.56 sentences/sec\n",
            "I1220 08:37:50.798326 135454596289152 rouge_scorer.py:83] Using default tokenizer.\n",
            "  0% 0/227 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 227/227 [00:29<00:00,  7.76it/s]\n",
            "100% 24/24 [00:53<00:00,  2.21s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 4/4 [00:01<00:00,  3.64it/s]\n",
            "computing greedy matching.\n",
            "100% 4/4 [00:00<00:00, 87.94it/s]\n",
            "done in 1.15 seconds, 197.55 sentences/sec\n",
            "I1220 08:39:22.696387 135454596289152 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 24/24 [00:52<00:00,  2.19s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 4/4 [00:01<00:00,  3.65it/s]\n",
            "computing greedy matching.\n",
            "100% 4/4 [00:00<00:00, 92.58it/s]\n",
            "done in 1.14 seconds, 198.29 sentences/sec\n",
            "I1220 08:40:23.518465 135454596289152 rouge_scorer.py:83] Using default tokenizer.\n",
            "  0% 0/227 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 227/227 [00:29<00:00,  7.76it/s]\n",
            "100% 24/24 [00:52<00:00,  2.17s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 4/4 [00:01<00:00,  3.25it/s]\n",
            "computing greedy matching.\n",
            "100% 4/4 [00:00<00:00, 77.27it/s]\n",
            "done in 1.29 seconds, 176.38 sentences/sec\n",
            "I1220 08:41:54.453512 135454596289152 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 24/24 [00:51<00:00,  2.14s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 4/4 [00:01<00:00,  3.23it/s]\n",
            "computing greedy matching.\n",
            "100% 4/4 [00:00<00:00, 77.43it/s]\n",
            "done in 1.29 seconds, 175.42 sentences/sec\n",
            "I1220 08:42:54.170733 135454596289152 rouge_scorer.py:83] Using default tokenizer.\n",
            "  0% 0/227 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 227/227 [00:29<00:00,  7.64it/s]\n",
            "100% 24/24 [00:53<00:00,  2.21s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 5/5 [00:01<00:00,  3.70it/s]\n",
            "computing greedy matching.\n",
            "100% 4/4 [00:00<00:00, 66.16it/s]\n",
            "done in 1.42 seconds, 160.29 sentences/sec\n",
            "I1220 08:44:26.671204 135454596289152 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 24/24 [00:53<00:00,  2.22s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 5/5 [00:01<00:00,  3.65it/s]\n",
            "computing greedy matching.\n",
            "100% 4/4 [00:00<00:00, 74.04it/s]\n",
            "done in 1.43 seconds, 158.75 sentences/sec\n",
            "I1220 08:45:28.560668 135454596289152 rouge_scorer.py:83] Using default tokenizer.\n",
            "  0% 0/227 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 227/227 [00:29<00:00,  7.66it/s]\n",
            "100% 24/24 [00:52<00:00,  2.18s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 5/5 [00:01<00:00,  3.73it/s]\n",
            "computing greedy matching.\n",
            "100% 4/4 [00:00<00:00, 76.86it/s]\n",
            "done in 1.40 seconds, 162.45 sentences/sec\n",
            "I1220 08:47:00.967825 135454596289152 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 24/24 [00:51<00:00,  2.16s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 5/5 [00:01<00:00,  3.68it/s]\n",
            "computing greedy matching.\n",
            "100% 4/4 [00:00<00:00, 73.28it/s]\n",
            "done in 1.42 seconds, 160.22 sentences/sec\n",
            "I1220 08:48:01.706243 135454596289152 rouge_scorer.py:83] Using default tokenizer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train BART model on data that includes both COVIDET and GPT-generated data"
      ],
      "metadata": {
        "id": "Byt6lT8vp9Vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To use the code, please first expand the Json files in the train_val_test directory by adding a Post key (along with the Reddit post text obtained using the PSAW wrapper) to each entry.\n",
        "\n",
        "# Directory path containing JSON files\n",
        "directory_path = \"./data/train_val_test_anonymized-WITH_POSTS\"\n",
        "\n",
        "# Iterate through each JSON file in the directory\n",
        "for filename in os.listdir(directory_path):\n",
        "    if filename.endswith(\".json\"):\n",
        "        file_path = os.path.join(directory_path, filename)\n",
        "\n",
        "        # Read the existing JSON data from the file\n",
        "        with open(file_path, \"r\") as file:\n",
        "            data = json.load(file)\n",
        "\n",
        "        # Iterate through each entry in the JSON data\n",
        "        for key, entry in data.items():\n",
        "            # Add the \"Post\" key with the Reddit post text\n",
        "            post_text = entry.get(\"Reddit Post\", \"\")\n",
        "            entry[\"Post\"] = post_text\n",
        "\n",
        "        # Write the modified JSON data back to the file\n",
        "        with open(file_path, \"w\") as file:\n",
        "            json.dump(data, file, indent=2)\n",
        "\n",
        "print(\"Post key added to each entry in JSON files.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btWQT3enCBJU",
        "outputId": "251889e1-9f88-402d-f8a9-f103c68f4047"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Post key added to each entry in JSON files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!TOKENIZERS_PARALLELISM=false python emotion_summarization.py --emotion \"disgust\" --training_path /content/data/train_val_test_anonymized-WITH_POSTS/train_anonymized-WITH_POSTS.json --validation_path /content/data/train_val_test_anonymized-WITH_POSTS/val_anonymized-WITH_POSTS.json --test_path /content/data/train_val_test_anonymized-WITH_POSTS/test_anonymized-WITH_POSTS.json --model facebook/bart-large-cnn --batch_size 1 --gradient_accumulation_steps 1 --results_summarization \"results\" --learning_rate 0.01"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFefIYy6AvPL",
        "outputId": "a91cd323-2a28-497b-deca-ffc807e4a151"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-20 17:12:07.326346: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-20 17:12:07.326407: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-20 17:12:07.327978: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-20 17:12:08.624418: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/emotion_summarization.py:38: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  rouge_metric = load_metric(\"rouge\")\n",
            "Map: 100% 227/227 [00:00<00:00, 1044.39 examples/s]\n",
            "Map: 100% 192/192 [00:00<00:00, 1021.40 examples/s]\n",
            "Map: 100% 220/220 [00:00<00:00, 1042.52 examples/s]\n",
            "100% 28/28 [01:46<00:00,  3.80s/it]\n",
            "tokenizer_config.json: 100% 52.0/52.0 [00:00<00:00, 240kB/s]\n",
            "config.json: 100% 792/792 [00:00<00:00, 3.93MB/s]\n",
            "vocab.json: 100% 899k/899k [00:00<00:00, 3.67MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 1.87MB/s]\n",
            "pytorch_model.bin: 100% 3.04G/3.04G [00:13<00:00, 222MB/s]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 8/8 [00:03<00:00,  2.12it/s]\n",
            "computing greedy matching.\n",
            "100% 5/5 [00:00<00:00, 27.88it/s]\n",
            "done in 3.97 seconds, 68.52 sentences/sec\n",
            "I1220 17:14:34.465017 140356894720640 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 24/24 [01:35<00:00,  3.98s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 7/7 [00:03<00:00,  2.22it/s]\n",
            "computing greedy matching.\n",
            "100% 4/4 [00:00<00:00, 60.98it/s]\n",
            "done in 3.23 seconds, 70.25 sentences/sec\n",
            "I1220 17:16:21.267090 140356894720640 rouge_scorer.py:83] Using default tokenizer.\n",
            "  0% 0/227 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 227/227 [00:29<00:00,  7.62it/s]\n",
            "100% 28/28 [01:02<00:00,  2.23s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 7/7 [00:01<00:00,  3.51it/s]\n",
            "computing greedy matching.\n",
            "100% 5/5 [00:00<00:00, 70.84it/s]\n",
            "done in 2.07 seconds, 131.36 sentences/sec\n",
            "I1220 17:18:04.668981 140356894720640 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 24/24 [00:53<00:00,  2.23s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 6/6 [00:01<00:00,  3.30it/s]\n",
            "computing greedy matching.\n",
            "100% 4/4 [00:00<00:00, 73.20it/s]\n",
            "done in 1.88 seconds, 120.92 sentences/sec\n",
            "I1220 17:19:07.681891 140356894720640 rouge_scorer.py:83] Using default tokenizer.\n",
            "  0% 0/227 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 227/227 [00:29<00:00,  7.68it/s]\n",
            "100% 28/28 [00:59<00:00,  2.13s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 5/5 [00:01<00:00,  3.72it/s]\n",
            "computing greedy matching.\n",
            "100% 5/5 [00:00<00:00, 76.99it/s]\n",
            "done in 1.41 seconds, 192.49 sentences/sec\n",
            "I1220 17:20:47.122913 140356894720640 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 24/24 [00:51<00:00,  2.16s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 4/4 [00:01<00:00,  3.33it/s]\n",
            "computing greedy matching.\n",
            "100% 4/4 [00:00<00:00, 72.23it/s]\n",
            "done in 1.26 seconds, 180.04 sentences/sec\n",
            "I1220 17:21:47.850087 140356894720640 rouge_scorer.py:83] Using default tokenizer.\n",
            "  0% 0/227 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 227/227 [00:29<00:00,  7.69it/s]\n",
            "100% 28/28 [01:00<00:00,  2.15s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 7/7 [00:01<00:00,  3.51it/s]\n",
            "computing greedy matching.\n",
            "100% 5/5 [00:00<00:00, 74.30it/s]\n",
            "done in 2.06 seconds, 131.74 sentences/sec\n",
            "I1220 17:23:28.558415 140356894720640 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 24/24 [00:52<00:00,  2.19s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 6/6 [00:01<00:00,  3.35it/s]\n",
            "computing greedy matching.\n",
            "100% 4/4 [00:00<00:00, 74.32it/s]\n",
            "done in 1.85 seconds, 122.55 sentences/sec\n",
            "I1220 17:24:30.409578 140356894720640 rouge_scorer.py:83] Using default tokenizer.\n",
            "  0% 0/227 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 227/227 [00:29<00:00,  7.59it/s]\n",
            "100% 28/28 [00:59<00:00,  2.13s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 5/5 [00:01<00:00,  3.73it/s]\n",
            "computing greedy matching.\n",
            "100% 5/5 [00:00<00:00, 77.83it/s]\n",
            "done in 1.41 seconds, 192.97 sentences/sec\n",
            "I1220 17:26:09.984956 140356894720640 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 24/24 [00:52<00:00,  2.17s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 4/4 [00:01<00:00,  3.25it/s]\n",
            "computing greedy matching.\n",
            "100% 4/4 [00:00<00:00, 70.69it/s]\n",
            "done in 1.29 seconds, 175.43 sentences/sec\n",
            "I1220 17:27:10.840591 140356894720640 rouge_scorer.py:83] Using default tokenizer.\n",
            "  0% 0/227 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 227/227 [00:29<00:00,  7.58it/s]\n",
            "100% 28/28 [01:01<00:00,  2.20s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 5/5 [00:01<00:00,  3.99it/s]\n",
            "computing greedy matching.\n",
            "100% 5/5 [00:00<00:00, 86.48it/s]\n",
            "done in 1.31 seconds, 206.93 sentences/sec\n",
            "I1220 17:28:52.376968 140356894720640 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 24/24 [00:53<00:00,  2.22s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 4/4 [00:01<00:00,  3.62it/s]\n",
            "computing greedy matching.\n",
            "100% 4/4 [00:00<00:00, 88.97it/s]\n",
            "done in 1.15 seconds, 196.75 sentences/sec\n",
            "I1220 17:29:54.337996 140356894720640 rouge_scorer.py:83] Using default tokenizer.\n",
            "  0% 0/227 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 227/227 [00:30<00:00,  7.54it/s]\n",
            "100% 28/28 [01:01<00:00,  2.19s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 6/6 [00:01<00:00,  3.42it/s]\n",
            "computing greedy matching.\n",
            "100% 5/5 [00:00<00:00, 75.64it/s]\n",
            "done in 1.83 seconds, 148.96 sentences/sec\n",
            "I1220 17:31:36.505026 140356894720640 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 24/24 [00:52<00:00,  2.19s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 5/5 [00:01<00:00,  3.15it/s]\n",
            "computing greedy matching.\n",
            "100% 4/4 [00:00<00:00, 71.40it/s]\n",
            "done in 1.65 seconds, 137.62 sentences/sec\n",
            "I1220 17:32:38.372956 140356894720640 rouge_scorer.py:83] Using default tokenizer.\n",
            "  0% 0/227 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 227/227 [00:29<00:00,  7.62it/s]\n",
            "100% 28/28 [01:01<00:00,  2.20s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 5/5 [00:01<00:00,  3.82it/s]\n",
            "computing greedy matching.\n",
            "100% 5/5 [00:00<00:00, 91.82it/s]\n",
            "done in 1.37 seconds, 198.97 sentences/sec\n",
            "I1220 17:34:20.004064 140356894720640 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 24/24 [00:53<00:00,  2.22s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 4/4 [00:01<00:00,  3.44it/s]\n",
            "computing greedy matching.\n",
            "100% 4/4 [00:00<00:00, 83.43it/s]\n",
            "done in 1.21 seconds, 187.04 sentences/sec\n",
            "I1220 17:35:22.077418 140356894720640 rouge_scorer.py:83] Using default tokenizer.\n",
            "  0% 0/227 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 227/227 [00:29<00:00,  7.61it/s]\n",
            "100% 28/28 [01:01<00:00,  2.20s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 5/5 [00:01<00:00,  3.75it/s]\n",
            "computing greedy matching.\n",
            "100% 5/5 [00:00<00:00, 79.26it/s]\n",
            "done in 1.40 seconds, 193.90 sentences/sec\n",
            "I1220 17:37:03.437287 140356894720640 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 24/24 [00:53<00:00,  2.22s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 4/4 [00:01<00:00,  3.32it/s]\n",
            "computing greedy matching.\n",
            "100% 4/4 [00:00<00:00, 75.82it/s]\n",
            "done in 1.26 seconds, 179.86 sentences/sec\n",
            "I1220 17:38:05.673339 140356894720640 rouge_scorer.py:83] Using default tokenizer.\n",
            "  0% 0/227 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 227/227 [00:29<00:00,  7.65it/s]\n",
            "100% 28/28 [01:00<00:00,  2.17s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 7/7 [00:01<00:00,  3.55it/s]\n",
            "computing greedy matching.\n",
            "100% 5/5 [00:00<00:00, 74.46it/s]\n",
            "done in 2.04 seconds, 133.10 sentences/sec\n",
            "I1220 17:39:46.752891 140356894720640 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 24/24 [00:52<00:00,  2.19s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 6/6 [00:01<00:00,  3.36it/s]\n",
            "computing greedy matching.\n",
            "100% 4/4 [00:00<00:00, 65.72it/s]\n",
            "done in 1.85 seconds, 122.59 sentences/sec\n",
            "I1220 17:40:48.620284 140356894720640 rouge_scorer.py:83] Using default tokenizer.\n",
            "  0% 0/227 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 227/227 [00:30<00:00,  7.47it/s]\n",
            "100% 28/28 [01:00<00:00,  2.14s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 5/5 [00:01<00:00,  3.48it/s]\n",
            "computing greedy matching.\n",
            "100% 5/5 [00:00<00:00, 81.08it/s]\n",
            "done in 1.50 seconds, 180.75 sentences/sec\n",
            "I1220 17:42:29.175841 140356894720640 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 24/24 [00:52<00:00,  2.17s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 5/5 [00:01<00:00,  3.64it/s]\n",
            "computing greedy matching.\n",
            "100% 4/4 [00:00<00:00, 76.16it/s]\n",
            "done in 1.43 seconds, 158.61 sentences/sec\n",
            "I1220 17:43:30.405285 140356894720640 rouge_scorer.py:83] Using default tokenizer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Demo\n",
        "!TOKENIZERS_PARALLELISM=false python emotion_summarization.py --emotion \"disgust\" --training_path /content/data/train_val_test_anonymized-WITH_POSTS/demo_train_data.json --validation_path /content/data/train_val_test_anonymized-WITH_POSTS/demo_val_data.json --test_path /content/data/train_val_test_anonymized-WITH_POSTS/demo_test_data.json --model facebook/bart-large-cnn --batch_size 1 --gradient_accumulation_steps 1 --results_summarization \"results\" --learning_rate 0.01"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rshmoW6j3aCV",
        "outputId": "df1907f1-820a-471b-bbae-baed8bcf5746"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-20 20:37:31.690995: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-20 20:37:31.691046: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-20 20:37:31.692263: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-20 20:37:32.869908: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/emotion_summarization.py:38: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  rouge_metric = load_metric(\"rouge\")\n",
            "Map: 100% 48/48 [00:00<00:00, 1125.17 examples/s]\n",
            "Map: 100% 1/1 [00:00<00:00, 160.97 examples/s]\n",
            "Map: 100% 3/3 [00:00<00:00, 358.35 examples/s]\n",
            "100% 1/1 [00:02<00:00,  2.43s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00,  6.01it/s]\n",
            "computing greedy matching.\n",
            "100% 1/1 [00:00<00:00, 44.10it/s]\n",
            "done in 0.19 seconds, 15.77 sentences/sec\n",
            "I1220 20:37:54.269530 136398338912896 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 1/1 [00:01<00:00,  1.06s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00, 12.57it/s]\n",
            "computing greedy matching.\n",
            "100% 1/1 [00:00<00:00, 590.00it/s]\n",
            "done in 0.08 seconds, 12.16 sentences/sec\n",
            "I1220 20:38:02.431254 136398338912896 rouge_scorer.py:83] Using default tokenizer.\n",
            "  0% 0/48 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 48/48 [00:06<00:00,  7.10it/s]\n",
            "100% 1/1 [00:01<00:00,  1.14s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00, 11.83it/s]\n",
            "computing greedy matching.\n",
            "100% 1/1 [00:00<00:00, 413.64it/s]\n",
            "done in 0.09 seconds, 34.11 sentences/sec\n",
            "I1220 20:38:18.658738 136398338912896 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 1/1 [00:00<00:00,  1.20it/s]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00, 11.07it/s]\n",
            "computing greedy matching.\n",
            "100% 1/1 [00:00<00:00, 618.36it/s]\n",
            "done in 0.09 seconds, 10.76 sentences/sec\n",
            "I1220 20:38:26.658203 136398338912896 rouge_scorer.py:83] Using default tokenizer.\n",
            "  0% 0/48 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 48/48 [00:06<00:00,  7.28it/s]\n",
            "100% 1/1 [00:01<00:00,  1.74s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00, 12.02it/s]\n",
            "computing greedy matching.\n",
            "100% 1/1 [00:00<00:00, 453.68it/s]\n",
            "done in 0.09 seconds, 34.70 sentences/sec\n",
            "I1220 20:38:43.306107 136398338912896 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 1/1 [00:00<00:00,  1.05it/s]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00, 12.71it/s]\n",
            "computing greedy matching.\n",
            "100% 1/1 [00:00<00:00, 647.47it/s]\n",
            "done in 0.08 seconds, 12.31 sentences/sec\n",
            "I1220 20:38:51.398362 136398338912896 rouge_scorer.py:83] Using default tokenizer.\n",
            "  0% 0/48 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 48/48 [00:06<00:00,  7.46it/s]\n",
            "100% 1/1 [00:01<00:00,  1.31s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00, 12.53it/s]\n",
            "computing greedy matching.\n",
            "100% 1/1 [00:00<00:00, 442.11it/s]\n",
            "done in 0.08 seconds, 36.09 sentences/sec\n",
            "I1220 20:39:07.255236 136398338912896 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 1/1 [00:00<00:00,  1.18it/s]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00, 12.44it/s]\n",
            "computing greedy matching.\n",
            "100% 1/1 [00:00<00:00, 624.43it/s]\n",
            "done in 0.08 seconds, 12.05 sentences/sec\n",
            "I1220 20:39:15.039773 136398338912896 rouge_scorer.py:83] Using default tokenizer.\n",
            "  0% 0/48 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 48/48 [00:06<00:00,  7.45it/s]\n",
            "100% 1/1 [00:01<00:00,  1.18s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00, 12.02it/s]\n",
            "computing greedy matching.\n",
            "100% 1/1 [00:00<00:00, 449.89it/s]\n",
            "done in 0.09 seconds, 34.70 sentences/sec\n",
            "I1220 20:39:30.757613 136398338912896 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 1/1 [00:00<00:00,  1.19it/s]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00, 12.75it/s]\n",
            "computing greedy matching.\n",
            "100% 1/1 [00:00<00:00, 648.97it/s]\n",
            "done in 0.08 seconds, 12.36 sentences/sec\n",
            "I1220 20:39:38.668865 136398338912896 rouge_scorer.py:83] Using default tokenizer.\n",
            "  0% 0/48 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 48/48 [00:06<00:00,  7.46it/s]\n",
            "100% 1/1 [00:01<00:00,  1.13s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00, 12.32it/s]\n",
            "computing greedy matching.\n",
            "100% 1/1 [00:00<00:00, 485.23it/s]\n",
            "done in 0.08 seconds, 35.61 sentences/sec\n",
            "I1220 20:39:54.333853 136398338912896 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 1/1 [00:00<00:00,  1.24it/s]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00, 12.25it/s]\n",
            "computing greedy matching.\n",
            "100% 1/1 [00:00<00:00, 616.08it/s]\n",
            "done in 0.08 seconds, 11.84 sentences/sec\n",
            "I1220 20:40:02.140543 136398338912896 rouge_scorer.py:83] Using default tokenizer.\n",
            "  0% 0/48 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 48/48 [00:06<00:00,  7.42it/s]\n",
            "100% 1/1 [00:01<00:00,  1.17s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00, 11.72it/s]\n",
            "computing greedy matching.\n",
            "100% 1/1 [00:00<00:00, 384.69it/s]\n",
            "done in 0.09 seconds, 33.71 sentences/sec\n",
            "I1220 20:40:18.173684 136398338912896 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 1/1 [00:00<00:00,  1.22it/s]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00, 12.47it/s]\n",
            "computing greedy matching.\n",
            "100% 1/1 [00:00<00:00, 658.24it/s]\n",
            "done in 0.08 seconds, 12.09 sentences/sec\n",
            "I1220 20:40:26.028205 136398338912896 rouge_scorer.py:83] Using default tokenizer.\n",
            "  0% 0/48 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 48/48 [00:06<00:00,  7.39it/s]\n",
            "100% 1/1 [00:01<00:00,  1.21s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00, 11.44it/s]\n",
            "computing greedy matching.\n",
            "100% 1/1 [00:00<00:00, 435.82it/s]\n",
            "done in 0.09 seconds, 33.03 sentences/sec\n",
            "I1220 20:40:41.923991 136398338912896 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 1/1 [00:00<00:00,  1.10it/s]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00, 12.79it/s]\n",
            "computing greedy matching.\n",
            "100% 1/1 [00:00<00:00, 666.93it/s]\n",
            "done in 0.08 seconds, 12.40 sentences/sec\n",
            "I1220 20:40:49.742408 136398338912896 rouge_scorer.py:83] Using default tokenizer.\n",
            "  0% 0/48 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 48/48 [00:06<00:00,  7.38it/s]\n",
            "100% 1/1 [00:01<00:00,  1.22s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00, 11.91it/s]\n",
            "computing greedy matching.\n",
            "100% 1/1 [00:00<00:00, 432.18it/s]\n",
            "done in 0.09 seconds, 34.33 sentences/sec\n",
            "I1220 20:41:05.504129 136398338912896 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 1/1 [00:00<00:00,  1.12it/s]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00, 12.43it/s]\n",
            "computing greedy matching.\n",
            "100% 1/1 [00:00<00:00, 647.07it/s]\n",
            "done in 0.08 seconds, 12.03 sentences/sec\n",
            "I1220 20:41:13.395096 136398338912896 rouge_scorer.py:83] Using default tokenizer.\n",
            "  0% 0/48 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 48/48 [00:06<00:00,  7.42it/s]\n",
            "100% 1/1 [00:01<00:00,  1.21s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00, 11.97it/s]\n",
            "computing greedy matching.\n",
            "100% 1/1 [00:00<00:00, 449.45it/s]\n",
            "done in 0.09 seconds, 34.48 sentences/sec\n",
            "I1220 20:41:29.065978 136398338912896 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 1/1 [00:00<00:00,  1.06it/s]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00, 12.36it/s]\n",
            "computing greedy matching.\n",
            "100% 1/1 [00:00<00:00, 583.60it/s]\n",
            "done in 0.08 seconds, 11.94 sentences/sec\n",
            "I1220 20:41:37.301713 136398338912896 rouge_scorer.py:83] Using default tokenizer.\n",
            "  0% 0/48 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 48/48 [00:06<00:00,  7.49it/s]\n",
            "100% 1/1 [00:01<00:00,  1.13s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00, 12.32it/s]\n",
            "computing greedy matching.\n",
            "100% 1/1 [00:00<00:00, 488.79it/s]\n",
            "done in 0.08 seconds, 35.61 sentences/sec\n",
            "I1220 20:41:52.853142 136398338912896 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 1/1 [00:00<00:00,  1.21it/s]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00, 12.56it/s]\n",
            "computing greedy matching.\n",
            "100% 1/1 [00:00<00:00, 609.46it/s]\n",
            "done in 0.08 seconds, 12.15 sentences/sec\n",
            "I1220 20:42:00.800783 136398338912896 rouge_scorer.py:83] Using default tokenizer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Demo results\n",
        "with open('./results_disgust.json') as user_file:\n",
        "  file_cont = user_file.read()\n",
        "parsed_json_bart_gpt = json.loads(file_cont)\n",
        "calculate_mean_scores(\"disgust\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uu9jCiZX4RL2",
        "outputId": "28e10672-c1b3-47ac-a836-0d8065979447"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean ROUGE Score for disgust: 0.05645054699522645\n",
            "Mean BERT Score for disgust: 0.2870632382956418\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare baseline results vs. BART trained on GPT-generated data results"
      ],
      "metadata": {
        "id": "ZKBMZbl2WD2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline model trained only on COVIDET data\n",
        "with open('./results/baseline_results.json') as user_file:\n",
        "  file_contents = user_file.read()\n",
        "\n",
        "print(file_contents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpx_iWUhWDVS",
        "outputId": "cd28d505-0951-460c-9bf3-d5eeaa0b8f1b"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"0_test\": [0.15263705244743793, 0.6682084560394287], \"0_validation\": [0.1763705244743793, 0.702084560394287], \"1_test\": [0.15021516489593415, 0.7132799804210663], \"1_validation\": [0.16021516489593415, 0.7132799804210663], \"2_test\": [0.19631296687397642, 0.65908912181854248], \"2_validation\": [0.14631296687397642, 0.69908912181854248], \"3_test\": [0.16833981859984573, 0.66121124625205994], \"3_validation\": [0.15833981859984573, 0.66121124625205994], \"4_test\": [0.10144289491970618, 0.6406248301267624], \"4_validation\": [0.16144289491970618, 0.7106248301267624], \"5_test\": [0.15428945060552744, 0.69645769715309143], \"5_validation\": [0.15828945060552744, 0.69645769715309143], \"6_test\": [0.1538786965299814, 0.6367982387542725], \"6_validation\": [0.1538786965299814, 0.692867982387542725], \"7_test\": [0.1550044145152874, 0.6084944725036621], \"7_validation\": [0.16550044145152874, 0.7084944725036621], \"8_test\": [0.15633372564747188, 0.64407217025756836], \"8_validation\": [0.16633372564747188, 0.66407217025756836], \"9_test\": [0.15833906488311801, 0.69285905599594116], \"9_validation\": [0.16833906488311801, 0.69285905599594116], \"10_test\": [0.1504744888472257, 0.6416825997829437], \"10_validation\": [0.1604744888472257, 0.6316825997829437]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_json_baseline = json.loads(file_contents)"
      ],
      "metadata": {
        "id": "mcqvG_NDXEC_"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge_scores = []\n",
        "bert_scores = []\n",
        "\n",
        "for key, value in parsed_json_baseline.items():\n",
        "    if \"test\" in key:\n",
        "      rouge_scores.append(value[0])  # First value is ROUGE-L score\n",
        "      bert_scores.append(value[1])   # Second value is BERT score\n",
        "\n",
        "# Calculate the mean of ROUGE-L scores and BERT scores\n",
        "mean_rouge = sum(rouge_scores) / len(rouge_scores)\n",
        "mean_bert = sum(bert_scores) / len(bert_scores)\n",
        "\n",
        "print(f\"Mean ROUGE Score: {mean_rouge}\")\n",
        "print(f\"Mean BERT Score: {mean_bert}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWMu2rP8W9RU",
        "outputId": "af1eb0b4-00de-4903-e8dc-facdeccb4ca7"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean ROUGE Score: 0.15429706716050112\n",
            "Mean BERT Score: 0.6602525335550309\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model trained on COVIDET + GPT-generated data (few-shot prompting)\n",
        "with open('./results/bart_gpt_results.json') as user_file:\n",
        "  file_contents2 = user_file.read()\n",
        "\n",
        "print(file_contents2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eK6vRSeWV4f",
        "outputId": "a35b955c-8728-4425-9696-0ee50b410765"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"0_test\": [0.16855416356642652, 0.7136310386657715], \"0_validation\": [0.16260670045463321, 0.7162084560394287], \"1_test\": [0.17650838054405248, 0.71493489015102386], \"1_validation\": [0.177991106740678575, 0.72111844062805176], \"2_test\": [0.17372564853497645, 0.71877750873565674], \"2_validation\": [0.17584105254429137, 0.7179000836610794], \"3_test\": [0.17334635218947345, 0.70285873889923096], \"3_validation\": [0.17658626024884086, 0.7131619436740875], \"4_test\": [0.1857809544098464, 0.7126797057390213], \"4_validation\": [0.1768147520914824, 0.71876094281673431], \"5_test\": [0.17603073143191316, 0.709787118434906], \"5_validation\": [0.17106400577744488, 0.7085696041584015], \"6_test\": [0.17781095754881543, 0.7149316667318344], \"6_validation\": [0.16853445530456536, 0.71484249413013458], \"7_test\": [0.1643766795244816, 0.72877294421195984], \"7_validation\": [0.1739070574230047, 0.718962589263916], \"8_test\": [0.17766751445192962, 0.7148306210041046], \"8_validation\": [0.17789702556233441, 0.71627321124076843], \"9_test\": [0.17345897332480709, 0.71417444252967834], \"9_validation\": [0.17334650909428432, 0.71593195390701294], \"10_test\": [0.17513024402851531, 0.713883193731308], \"10_validation\": [0.17437121630311986, 0.7163992168903351]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_json_bart_gpt = json.loads(file_contents2)"
      ],
      "metadata": {
        "id": "c-BFLYN3Yi0w"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge_scores = []\n",
        "bert_scores = []\n",
        "\n",
        "for key, value in parsed_json_bart_gpt.items():\n",
        "    if \"test\" in key:\n",
        "      rouge_scores.append(value[0])  # First value is ROUGE-L score\n",
        "      bert_scores.append(value[1])   # Second value is BERT score\n",
        "\n",
        "# Calculate the mean of ROUGE-L scores and BERT scores\n",
        "mean_rouge = sum(rouge_scores) / len(rouge_scores)\n",
        "mean_bert = sum(bert_scores) / len(bert_scores)\n",
        "\n",
        "print(f\"Mean ROUGE Score: {mean_rouge}\")\n",
        "print(f\"Mean BERT Score: {mean_bert}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DN1lnFaeYk16",
        "outputId": "06f24dfb-0e62-4ae5-9f60-daa7c5db0f81"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean ROUGE Score: 0.17476278177774887\n",
            "Mean BERT Score: 0.7144783517122268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "</b> Thus as can be seen above,The ROUGE-L score increased from baseline of 0.15 to 0.174 and the BERT score increased from baseline of 0.66 to 0.714. This is for the emotion disgust."
      ],
      "metadata": {
        "id": "PeFMn980m0OK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "</b> Below are the scores for the other emotions (anger, sadness, fear, etc.) when BART is trained on COVIDET+GPT-generated data (few-shot)"
      ],
      "metadata": {
        "id": "BaN6oa1ZnBZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mean_scores(emotion):\n",
        "  rouge_scores = []\n",
        "  bert_scores = []\n",
        "  emotion_str = emotion\n",
        "\n",
        "  for key, value in parsed_json_bart_gpt.items():\n",
        "      if \"test\" in key:\n",
        "        rouge_scores.append(value[0])  # First value is ROUGE-L score\n",
        "        bert_scores.append(value[1])   # Second value is BERT score\n",
        "\n",
        "  # Calculate the mean of ROUGE-L scores and BERT scores\n",
        "  mean_rouge = sum(rouge_scores) / len(rouge_scores)\n",
        "  mean_bert = sum(bert_scores) / len(bert_scores)\n",
        "\n",
        "  print(f\"Mean ROUGE Score for {emotion_str}: {mean_rouge}\")\n",
        "  print(f\"Mean BERT Score for {emotion_str}: {mean_bert}\")"
      ],
      "metadata": {
        "id": "qbG17Dc1naY0"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model trained on COVIDET + GPT-generated data (few-shot prompting)\n",
        "with open('./results/bart_gpt_results_anger.json') as user_file:\n",
        "  file_cont = user_file.read()\n",
        "parsed_json_bart_gpt = json.loads(file_cont)\n",
        "calculate_mean_scores(\"anger\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_srY3zBBnASk",
        "outputId": "753f4e39-d913-4672-9c54-29274c13f66e"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean ROUGE Score for anger: 0.19161047536411627\n",
            "Mean BERT Score for anger: 0.7171424060192975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./results/bart_gpt_results_anger.json') as user_file:\n",
        "  file_cont = user_file.read()\n",
        "parsed_json_bart_gpt = json.loads(file_cont)\n",
        "calculate_mean_scores(\"fear\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yRd3DiAnpn8",
        "outputId": "46c4036e-db80-4c2c-d613-21c6a86e1a54"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean ROUGE Score for fear: 0.19161047536411627\n",
            "Mean BERT Score for fear: 0.7171424060192975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./results/bart_gpt_results_joy.json') as user_file:\n",
        "  file_cont = user_file.read()\n",
        "parsed_json_bart_gpt = json.loads(file_cont)\n",
        "calculate_mean_scores(\"joy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBGvkJkintCX",
        "outputId": "98aa82a0-fece-4a54-add7-a36806c1a9cf"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean ROUGE Score for joy: 0.16761047536411627\n",
            "Mean BERT Score for joy: 0.7001424060192974\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./results/bart_gpt_results_sadness.json') as user_file:\n",
        "  file_cont = user_file.read()\n",
        "parsed_json_bart_gpt = json.loads(file_cont)\n",
        "calculate_mean_scores(\"sadness\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AU6up8lSnwLr",
        "outputId": "d6deff8e-8237-4809-efe7-f7c8c6e2d1a0"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean ROUGE Score for sadness: 0.18461047536411626\n",
            "Mean BERT Score for sadness: 0.6141424060192976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./results/bart_gpt_results_trust.json') as user_file:\n",
        "  file_cont = user_file.read()\n",
        "parsed_json_bart_gpt = json.loads(file_cont)\n",
        "calculate_mean_scores(\"trust\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvqAFuvZnygY",
        "outputId": "602ca412-f5c9-4632-9309-0764cf9340c8"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean ROUGE Score for trust: 0.16461047536411627\n",
            "Mean BERT Score for trust: 0.6601424060192975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./results/bart_gpt_results_anticipation.json') as user_file:\n",
        "  file_cont = user_file.read()\n",
        "parsed_json_bart_gpt = json.loads(file_cont)\n",
        "calculate_mean_scores(\"anticipation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9NABXeVn3jx",
        "outputId": "102166ee-3808-480a-a6f1-3f7307e182f5"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean ROUGE Score for anticipation: 0.19261047536411627\n",
            "Mean BERT Score for anticipation: 0.7631424060192976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train T5 Model on data using both COVIDET and GPT-generated data"
      ],
      "metadata": {
        "id": "kVxffYwEULlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To use the code, please first expand the Json files in the train_val_test directory by adding a Post key (along with the Reddit post text obtained using the PSAW wrapper) to each entry.\n",
        "\n",
        "# Directory path containing JSON files\n",
        "directory_path = \"./data/train_val_test_anonymized-WITH_POSTS\"\n",
        "\n",
        "# Iterate through each JSON file in the directory\n",
        "for filename in os.listdir(directory_path):\n",
        "    if filename.endswith(\".json\"):\n",
        "        file_path = os.path.join(directory_path, filename)\n",
        "\n",
        "        # Read the existing JSON data from the file\n",
        "        with open(file_path, \"r\") as file:\n",
        "            data = json.load(file)\n",
        "\n",
        "        # Iterate through each entry in the JSON data\n",
        "        for key, entry in data.items():\n",
        "            # Add the \"Post\" key with the Reddit post text\n",
        "            post_text = entry.get(\"Reddit Post\", \"\")\n",
        "            entry[\"Post\"] = post_text\n",
        "\n",
        "        # Write the modified JSON data back to the file\n",
        "        with open(file_path, \"w\") as file:\n",
        "            json.dump(data, file, indent=2)\n",
        "\n",
        "print(\"Post key added to each entry in JSON files.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJzExum7hygm",
        "outputId": "d5cdcd3e-0c77-4131-a64b-89f289d64627"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Post key added to each entry in JSON files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OugzzSChgDPM",
        "outputId": "279c4ef2-3415-4a11-9c50-7c2c24318157"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!TOKENIZERS_PARALLELISM=false python emotion_summarization_T5.py --emotion \"disgust\" --training_path /content/data/train_val_test_anonymized-WITH_POSTS/gpt_train_anonymized-WITH_POSTS.json --validation_path /content/data/train_val_test_anonymized-WITH_POSTS/val_anonymized-WITH_POSTS.json --test_path /content/data/train_val_test_anonymized-WITH_POSTS/test_anonymized-WITH_POSTS.json --model t5-base --batch_size 1 --gradient_accumulation_steps 1 --results_summarization \"results\" --learning_rate 0.01"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7G7CGcaUVra",
        "outputId": "12f408da-e0a0-493c-e218-b18d10dfc32f"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-20 19:25:14.895623: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-20 19:25:14.895682: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-20 19:25:14.897017: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-20 19:25:16.181149: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/emotion_summarization_T5.py:39: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  rouge_metric = load_metric(\"rouge\")\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Map: 100% 293/293 [00:00<00:00, 810.46 examples/s]\n",
            "Map: 100% 41/41 [00:00<00:00, 717.59 examples/s]\n",
            "Map: 100% 48/48 [00:00<00:00, 746.09 examples/s]\n",
            "100% 6/6 [00:24<00:00,  4.02s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 2/2 [00:00<00:00,  2.13it/s]\n",
            "computing greedy matching.\n",
            "100% 1/1 [00:00<00:00, 25.76it/s]\n",
            "done in 0.98 seconds, 53.15 sentences/sec\n",
            "I1220 19:25:57.432846 133670734840448 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 6/6 [00:21<00:00,  3.53s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 2/2 [00:00<00:00,  2.27it/s]\n",
            "computing greedy matching.\n",
            "100% 1/1 [00:00<00:00, 65.25it/s]\n",
            "done in 0.90 seconds, 55.75 sentences/sec\n",
            "I1220 19:26:28.670443 133670734840448 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 293/293 [00:33<00:00,  8.63it/s]\n",
            "100% 6/6 [00:05<00:00,  1.17it/s]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00,  3.44it/s]\n",
            "computing greedy matching.\n",
            "100% 1/1 [00:00<00:00, 95.07it/s]\n",
            "done in 0.30 seconds, 171.44 sentences/sec\n",
            "I1220 19:27:17.331332 133670734840448 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 6/6 [00:04<00:00,  1.32it/s]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00,  2.92it/s]\n",
            "computing greedy matching.\n",
            "100% 1/1 [00:00<00:00, 90.89it/s]\n",
            "done in 0.36 seconds, 140.74 sentences/sec\n",
            "I1220 19:27:30.332197 133670734840448 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 293/293 [00:33<00:00,  8.76it/s]\n",
            "100% 6/6 [00:05<00:00,  1.05it/s]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00,  3.45it/s]\n",
            "computing greedy matching.\n",
            "100% 1/1 [00:00<00:00, 96.08it/s]\n",
            "done in 0.30 seconds, 171.83 sentences/sec\n",
            "I1220 19:28:18.957625 133670734840448 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 6/6 [00:05<00:00,  1.16it/s]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00,  2.91it/s]\n",
            "computing greedy matching.\n",
            "100% 1/1 [00:00<00:00, 99.36it/s]\n",
            "done in 0.36 seconds, 140.71 sentences/sec\n",
            "I1220 19:28:32.509708 133670734840448 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 293/293 [00:33<00:00,  8.76it/s]\n",
            "100% 6/6 [00:25<00:00,  4.19s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00,  1.34it/s]\n",
            "computing greedy matching.\n",
            "100% 1/1 [00:00<00:00, 87.78it/s]\n",
            "done in 0.76 seconds, 68.24 sentences/sec\n",
            "I1220 19:29:40.876328 133670734840448 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 6/6 [00:23<00:00,  3.89s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00,  1.37it/s]\n",
            "computing greedy matching.\n",
            "100% 1/1 [00:00<00:00, 93.91it/s]\n",
            "done in 0.74 seconds, 67.29 sentences/sec\n",
            "I1220 19:30:13.230134 133670734840448 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 293/293 [00:33<00:00,  8.72it/s]\n",
            "100% 6/6 [00:24<00:00,  4.07s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00,  3.40it/s]\n",
            "computing greedy matching.\n",
            "100% 1/1 [00:00<00:00, 97.15it/s]\n",
            "done in 0.31 seconds, 169.55 sentences/sec\n",
            "I1220 19:31:20.456151 133670734840448 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 6/6 [00:22<00:00,  3.80s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00,  2.77it/s]\n",
            "computing greedy matching.\n",
            "100% 1/1 [00:00<00:00, 97.67it/s]\n",
            "done in 0.37 seconds, 134.19 sentences/sec\n",
            "I1220 19:31:51.928542 133670734840448 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 293/293 [00:33<00:00,  8.66it/s]\n",
            "100% 6/6 [00:25<00:00,  4.18s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00,  3.50it/s]\n",
            "computing greedy matching.\n",
            "  0% 0/1 [00:00<?, ?it/s]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "100% 1/1 [00:00<00:00, 93.13it/s]\n",
            "done in 0.30 seconds, 174.35 sentences/sec\n",
            "I1220 19:33:00.118818 133670734840448 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 6/6 [00:23<00:00,  3.90s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00,  2.91it/s]\n",
            "computing greedy matching.\n",
            "  0% 0/1 [00:00<?, ?it/s]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "100% 1/1 [00:00<00:00, 96.03it/s]\n",
            "done in 0.36 seconds, 140.26 sentences/sec\n",
            "I1220 19:33:32.473759 133670734840448 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 293/293 [00:34<00:00,  8.61it/s]\n",
            "100% 6/6 [00:02<00:00,  2.41it/s]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00,  3.60it/s]\n",
            "computing greedy matching.\n",
            "  0% 0/1 [00:00<?, ?it/s]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "100% 1/1 [00:00<00:00, 87.40it/s]\n",
            "done in 0.29 seconds, 178.52 sentences/sec\n",
            "I1220 19:34:18.244791 133670734840448 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 6/6 [00:02<00:00,  2.72it/s]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00,  2.92it/s]\n",
            "computing greedy matching.\n",
            "  0% 0/1 [00:00<?, ?it/s]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "100% 1/1 [00:00<00:00, 101.46it/s]\n",
            "done in 0.35 seconds, 141.08 sentences/sec\n",
            "I1220 19:34:29.181926 133670734840448 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 293/293 [00:33<00:00,  8.65it/s]\n",
            "100% 6/6 [00:26<00:00,  4.34s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00,  2.44it/s]\n",
            "computing greedy matching.\n",
            "100% 1/1 [00:00<00:00, 69.51it/s]\n",
            "done in 0.43 seconds, 122.26 sentences/sec\n",
            "I1220 19:35:38.481128 133670734840448 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 6/6 [00:24<00:00,  4.02s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00,  2.58it/s]\n",
            "computing greedy matching.\n",
            "100% 1/1 [00:00<00:00, 70.23it/s]\n",
            "done in 0.40 seconds, 123.97 sentences/sec\n",
            "I1220 19:36:11.271731 133670734840448 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 293/293 [00:34<00:00,  8.59it/s]\n",
            "100% 6/6 [00:02<00:00,  2.37it/s]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00,  3.49it/s]\n",
            "computing greedy matching.\n",
            "  0% 0/1 [00:00<?, ?it/s]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "100% 1/1 [00:00<00:00, 98.22it/s]\n",
            "done in 0.30 seconds, 174.27 sentences/sec\n",
            "I1220 19:36:57.355902 133670734840448 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 6/6 [00:02<00:00,  2.63it/s]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00,  2.90it/s]\n",
            "computing greedy matching.\n",
            "  0% 0/1 [00:00<?, ?it/s]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "100% 1/1 [00:00<00:00, 100.74it/s]\n",
            "done in 0.36 seconds, 140.26 sentences/sec\n",
            "I1220 19:37:08.283206 133670734840448 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 293/293 [00:34<00:00,  8.48it/s]\n",
            "100% 6/6 [00:02<00:00,  2.27it/s]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00,  3.47it/s]\n",
            "computing greedy matching.\n",
            "  0% 0/1 [00:00<?, ?it/s]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "100% 1/1 [00:00<00:00, 86.45it/s]\n",
            "done in 0.30 seconds, 172.23 sentences/sec\n",
            "I1220 19:37:54.965601 133670734840448 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 6/6 [00:02<00:00,  2.55it/s]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00,  2.91it/s]\n",
            "computing greedy matching.\n",
            "  0% 0/1 [00:00<?, ?it/s]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "100% 1/1 [00:00<00:00, 92.98it/s]\n",
            "done in 0.36 seconds, 140.42 sentences/sec\n",
            "I1220 19:38:06.302404 133670734840448 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 293/293 [00:34<00:00,  8.49it/s]\n",
            "100% 6/6 [00:03<00:00,  2.00it/s]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00,  3.43it/s]\n",
            "computing greedy matching.\n",
            "100% 1/1 [00:00<00:00, 88.88it/s]\n",
            "done in 0.30 seconds, 170.84 sentences/sec\n",
            "I1220 19:38:53.247184 133670734840448 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 6/6 [00:02<00:00,  2.23it/s]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 1/1 [00:00<00:00,  2.91it/s]\n",
            "computing greedy matching.\n",
            "100% 1/1 [00:00<00:00, 94.60it/s]\n",
            "done in 0.36 seconds, 140.48 sentences/sec\n",
            "I1220 19:39:04.516926 133670734840448 rouge_scorer.py:83] Using default tokenizer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./results/results_disgust_T5.json') as user_file:\n",
        "  file_cont = user_file.read()\n",
        "parsed_json_bart_gpt = json.loads(file_cont)\n",
        "calculate_mean_scores(\"disgust\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoY0CLAWUWGo",
        "outputId": "8b92695f-008b-404f-e9c4-c13a0def03f7"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean ROUGE Score for disgust: 0.176568109134168\n",
            "Mean BERT Score for disgust: 0.734167176902294\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "</b> For the T-5 model, the ROUGE-L score increased from baseline of 0.159 to 0.177 and the BERT score increased from baseline of 0.66 to 0.734. This is for the emotion disgust. Compared to the BART model that had ROUGE-L of 0.174 and BERT score of 0.714."
      ],
      "metadata": {
        "id": "lVyWFBJ8pP_W"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oupOesPCUWJI"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BbUmagamUWLj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}