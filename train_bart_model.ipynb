{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Training and testing on original COVIDET dataset without including GPT-generated data"
      ],
      "metadata": {
        "id": "t92zwpTabSZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert-score"
      ],
      "metadata": {
        "id": "lPN2tGmCbWtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "hfswV1mzbWwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "id": "Ik73Pox6bWyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install psaw"
      ],
      "metadata": {
        "id": "xIGs7rQfbW1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bert_score import score as bert_scr\n",
        "import pandas as pd\n",
        "from absl import app\n",
        "from absl import flags\n",
        "\n",
        "from datasets import load_metric\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoModel, AutoTokenizer, DataCollatorForSeq2Seq, BartForConditionalGeneration\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "bgq1yJ4cbW3W"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To use the code, please first expand the Json files in the train_val_test directory by adding a Post key (along with the Reddit post text obtained using the PSAW wrapper) to each entry.\n",
        "\n",
        "# Directory path containing JSON files\n",
        "directory_path = \"./data/train_val_test_anonymized-WITH_POSTS\"\n",
        "\n",
        "# Iterate through each JSON file in the directory\n",
        "for filename in os.listdir(directory_path):\n",
        "    if filename.endswith(\".json\"):\n",
        "        file_path = os.path.join(directory_path, filename)\n",
        "\n",
        "        # Read the existing JSON data from the file\n",
        "        with open(file_path, \"r\") as file:\n",
        "            data = json.load(file)\n",
        "\n",
        "        # Iterate through each entry in the JSON data\n",
        "        for key, entry in data.items():\n",
        "            # Add the \"Post\" key with the Reddit post text\n",
        "            post_text = entry.get(\"Reddit Post\", \"\")\n",
        "            entry[\"Post\"] = post_text\n",
        "\n",
        "        # Write the modified JSON data back to the file\n",
        "        with open(file_path, \"w\") as file:\n",
        "            json.dump(data, file, indent=2)\n",
        "\n",
        "print(\"Post key added to each entry in JSON files.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7a_PfxlbeHr",
        "outputId": "26e2dab6-f4eb-4cfc-e47c-c6a8fcd46aa9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Post key added to each entry in JSON files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "3UrQiQeFf2Za"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!export 'PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512'"
      ],
      "metadata": {
        "id": "Qtdb8i8zn9Ms"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!TOKENIZERS_PARALLELISM=false python detection_summarization.py --emotion \"disgust\" --training_path /content/data/train_val_test_anonymized-WITH_POSTS/train_anonymized-WITH_POSTS.json --validation_path /content/data/train_val_test_anonymized-WITH_POSTS/train_anonymized-WITH_POSTS.json --test_path /content/data/train_val_test_anonymized-WITH_POSTS/train_anonymized-WITH_POSTS.json --model facebook/bart-large-cnn --batch_size 1 --gradient_accumulation_steps 1 --results_detection_summarization \"results\" --learning_rate 0.01"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hi4Xvk3kbeKY",
        "outputId": "9432f1a6-fa8c-4370-98fb-02ec65c3c0aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-16 21:55:43.510353: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-16 21:55:43.510410: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-16 21:55:43.511591: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-16 21:55:44.744265: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/detection_summarization.py:43: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  rouge_metric = load_metric(\"rouge\")\n",
            "Map: 100% 197/197 [00:00<00:00, 1072.05 examples/s]\n",
            "Map: 100% 834/834 [00:00<00:00, 1595.55 examples/s]\n",
            "Map: 100% 166/166 [00:00<00:00, 404.19 examples/s]\n",
            "Map: 100% 834/834 [00:00<00:00, 1204.74 examples/s]\n",
            "Map: 100% 166/166 [00:00<00:00, 1041.47 examples/s]\n",
            "Map: 100% 834/834 [00:00<00:00, 1178.15 examples/s]\n",
            "Some weights of DetectionSummarizationModel were not initialized from the model checkpoint at facebook/bart-large-cnn and are newly initialized: ['classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.dense.bias', 'classification_head.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 166/166 [00:08<00:00, 19.03it/s]\n",
            "100% 834/834 [00:38<00:00, 21.85it/s]\n",
            "100% 21/21 [01:18<00:00,  3.73s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 6/6 [00:02<00:00,  2.20it/s]\n",
            "computing greedy matching.\n",
            "100% 4/4 [00:00<00:00, 57.94it/s]\n",
            "done in 2.81 seconds, 70.21 sentences/sec\n",
            "I1216 21:58:13.627907 137785729487488 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 166/166 [00:08<00:00, 20.54it/s]\n",
            "100% 834/834 [00:38<00:00, 21.59it/s]\n",
            "100% 21/21 [01:23<00:00,  3.98s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 6/6 [00:02<00:00,  2.24it/s]\n",
            "computing greedy matching.\n",
            "100% 4/4 [00:00<00:00, 67.58it/s]\n",
            "done in 2.74 seconds, 71.90 sentences/sec\n",
            "I1216 22:00:35.536732 137785729487488 rouge_scorer.py:83] Using default tokenizer.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "0it [00:00, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "197it [01:08,  2.87it/s]\n",
            "100% 166/166 [00:08<00:00, 20.41it/s]\n",
            "100% 834/834 [00:38<00:00, 21.61it/s]\n",
            "100% 21/21 [01:11<00:00,  3.40s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 6/6 [00:01<00:00,  3.10it/s]\n",
            "computing greedy matching.\n",
            "100% 4/4 [00:00<00:00, 75.67it/s]\n",
            "done in 1.99 seconds, 98.99 sentences/sec\n",
            "I1216 22:03:53.917895 137785729487488 rouge_scorer.py:83] Using default tokenizer.\n",
            "100% 166/166 [00:08<00:00, 20.57it/s]\n",
            "100% 834/834 [00:38<00:00, 21.83it/s]\n",
            "100% 21/21 [01:10<00:00,  3.35s/it]\n",
            "calculating scores...\n",
            "computing bert embedding.\n",
            "100% 6/6 [00:01<00:00,  3.11it/s]\n",
            "computing greedy matching.\n",
            "100% 4/4 [00:00<00:00, 78.30it/s]\n",
            "done in 1.98 seconds, 99.26 sentences/sec\n",
            "I1216 22:06:01.289922 137785729487488 rouge_scorer.py:83] Using default tokenizer.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "0it [00:00, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "66it [00:22,  2.93it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!TOKENIZERS_PARALLELISM=false python emotion_summarization.py --emotion \"disgust\" --training_path /content/data/train_val_test_anonymized-WITH_POSTS/train_anonymized-WITH_POSTS.json --validation_path /content/data/train_val_test_anonymized-WITH_POSTS/train_anonymized-WITH_POSTS.json --test_path /content/data/train_val_test_anonymized-WITH_POSTS/train_anonymized-WITH_POSTS.json --model facebook/bart-large-cnn --batch_size 1 --gradient_accumulation_steps 1 --results_detection_summarization \"results\" --learning_rate 0.01"
      ],
      "metadata": {
        "id": "_T__96kKbePX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Jvt6_f2beR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g3QWqmMWbeUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train BART model on data that includes both COVIDET and GPT-generated data"
      ],
      "metadata": {
        "id": "Byt6lT8vp9Vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!TOKENIZERS_PARALLELISM=false python detection_summarization.py --emotion \"disgust\" --training_path /content/data/gpt_train_val_test_anonymized-WITH_POSTS/train_anonymized-WITH_POSTS.json --validation_path /content/data/train_val_test_anonymized-WITH_POSTS/train_anonymized-WITH_POSTS.json --test_path /content/data/train_val_test_anonymized-WITH_POSTS/train_anonymized-WITH_POSTS.json --model facebook/bart-large-cnn --batch_size 1 --gradient_accumulation_steps 1 --results_detection_summarization \"results\" --learning_rate 0.01"
      ],
      "metadata": {
        "id": "gv3iukTmbeWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4Ddndn8LbeY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PPz9cLZjbebO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}